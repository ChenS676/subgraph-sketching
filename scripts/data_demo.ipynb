{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba59b1dbac89a68",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Enviorment Installation \n",
    "In order to load dataset and run the code, you need to install the following packages:\n",
    "Dependencies (with python >= 3.9): Main dependencies are\n",
    "pytorch==1.13\n",
    "torch_geometric==2.2.0\n",
    "torch-scatter==2.1.1+pt113cpu\n",
    "torch-sparse==0.6.17+pt113cpu\n",
    "torch-spline-conv==1.2.2+pt113cpu\n",
    "\n",
    "# Dataset Download \n",
    "There are three available datasets for link prediction: Cora, Pubmed and Arxiv. Please download them from the following link:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "699679a4b0e356e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T11:01:16.690421Z",
     "start_time": "2023-09-25T11:01:16.674493Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kit/aifb/cc7738/anaconda3/envs/ss/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from yacs.config import CfgNode as CN\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import os, sys \n",
    "import pandas as pd \n",
    "import json\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "cfg = CN()\n",
    "\n",
    "cfg.dataset = CN()\n",
    "cfg.dataset.cora = CN()\n",
    "cfg.dataset.cora.root = '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE' #'PATH_TO_DATASET' \n",
    "cfg.dataset.cora.original = cfg.dataset.cora.root + '/dataset/cora_orig/cora'\n",
    "cfg.dataset.cora.papers =  cfg.dataset.cora.root + '/dataset/cora_orig/mccallum/cora/papers'\n",
    "cfg.dataset.cora.extractions =  cfg.dataset.cora.root + '/dataset/cora_andrew_mccallum/extractions/'\n",
    "cfg.dataset.cora.lm_model_name = 'microsoft/deberta-base'\n",
    "# ------------------------------------------------------------------------ #\n",
    "cfg.dataset.pubmed = CN()\n",
    "cfg.dataset.pubmed.root = '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE' #'PATH_TO_DATASET' \n",
    "cfg.dataset.pubmed.original = cfg.dataset.pubmed.root  + '/dataset/PubMed_orig/data/'\n",
    "cfg.dataset.pubmed.abs_ti = cfg.dataset.pubmed.root  + '/dataset/PubMed_orig/pubmed.json' \n",
    "\n",
    "cfg.dataset.arxiv = CN()\n",
    "cfg.dataset.arxiv.root = '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE' #'PATH_TO_DATASET' \n",
    "cfg.dataset.arxiv.abs_ti = cfg.dataset.arxiv.root + '/dataset/ogbn_arxiv_orig/titleabs.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eec148f590c63d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T09:02:53.852292Z",
     "start_time": "2023-09-25T09:02:53.826469Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'dataset': CfgNode({'cora': CfgNode({'root': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE', 'original': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE/dataset/cora_orig/cora', 'papers': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE/dataset/cora_orig/mccallum/cora/papers', 'extractions': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE/dataset/cora_andrew_mccallum/extractions/', 'lm_model_name': 'microsoft/deberta-base'}), 'pubmed': CfgNode({'root': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE', 'original': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE/dataset/PubMed_orig/data/', 'abs_ti': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE/dataset/PubMed_orig/pubmed.json'}), 'arxiv': CfgNode({'root': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE', 'abs_ti': '/pfs/work7/workspace/scratch/cc7738-prefeature/TAPE/dataset/ogbn_arxiv_orig/titleabs.tsv'})})})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159de8ae31274dd3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seed_everything(SEED=0):\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)  # Numpy module.\n",
    "    random.seed(SEED)  # Python random module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ec6a5fb2c36d25",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_ab_ti(path, fn):\n",
    "    ti, ab = '', ''\n",
    "    with open(path + fn) as f:\n",
    "        lines = f.read().splitlines()\n",
    "    for line in lines:\n",
    "        if line.split(':')[0] == 'Title':\n",
    "            ti = line\n",
    "        elif line.split(':')[0] == 'Abstract':\n",
    "            ab = line\n",
    "    return ti, ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df6f1d693f2cba8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-25T09:02:54.495451Z",
     "start_time": "2023-09-25T09:02:54.287303Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_raw_text_cora(cfg, use_text=False, seed=0):\n",
    "    # load data \n",
    "    path_papers = cfg.dataset.cora.papers\n",
    "    andrew_maccallum_path = cfg.dataset.cora.extractions \n",
    "    dataset = Planetoid(cfg.dataset.cora.root, 'cora',\n",
    "                        transform=T.NormalizeFeatures())\n",
    "    data = dataset[0]\n",
    "    print(data)\n",
    "\n",
    "    # load data_citeid \n",
    "    path = cfg.dataset.cora.original\n",
    "    idx_features_labels = np.genfromtxt(\n",
    "        \"{}.content\".format(path), dtype=np.dtype(str))\n",
    "    data_X = idx_features_labels[:, 1:-1].astype(np.float32)\n",
    "    labels = idx_features_labels[:, -1]\n",
    "    data_citeid = idx_features_labels[:, 0]\n",
    "    \n",
    "    if not use_text:\n",
    "        return data, None\n",
    "\n",
    "    with open(path_papers) as f:\n",
    "        lines = f.readlines()\n",
    "    pid_filename = {}\n",
    "    for line in lines:\n",
    "        pid = line.split('\\t')[0]\n",
    "        fn = line.split('\\t')[1]\n",
    "        pid_filename[pid] = fn\n",
    "\n",
    "    text = []\n",
    "    whole, founded = len(data_citeid), 0\n",
    "    no_ab_or_ti = 0\n",
    "    for pid in data_citeid:\n",
    "        fn = pid_filename[pid]\n",
    "        ti, ab = load_ab_ti(andrew_maccallum_path, fn)\n",
    "        founded += 1\n",
    "        text.append(ti + '\\n' + ab)\n",
    "\n",
    "        if ti == '' or ab == '':\n",
    "            # print(f\"no title {ti}, no abstract {ab}\")\n",
    "            no_ab_or_ti += 1\n",
    "    print(f\"found {founded}/{whole} papers, {no_ab_or_ti} no ab or ti.\")\n",
    "    return data, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "683453caa9c292cf",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "found 2708/2708 papers, 321 no ab or ti.\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    }
   ],
   "source": [
    "data, text = get_raw_text_cora(cfg, use_text=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68a6e0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: The megaprior heuristic for discovering protein sequence patterns  \n",
      "Abstract: Several computer algorithms for discovering patterns in groups of protein sequences are in use that are based on fitting the parameters of a statistical model to a group of related sequences. These include hidden Markov model (HMM) algorithms for multiple sequence alignment, and the MEME and Gibbs sampler algorithms for discovering motifs. These algorithms are sometimes prone to producing models that are incorrect because two or more patterns have been combined. The statistical model produced in this situation is a convex combination (weighted average) of two or more different models. This paper presents a solution to the problem of convex combinations in the form of a heuristic based on using extremely low variance Dirichlet mixture priors as part of the statistical model. This heuristic, which we call the megaprior heuristic, increases the strength (i.e., decreases the variance) of the prior in proportion to the size of the sequence dataset. This causes each column in the final model to strongly resemble the mean of a single component of the prior, regardless of the size of the dataset. We describe the cause of the convex combination problem, analyze it mathematically, motivate and describe the implementation of the megaprior heuristic, and show how it can effectively eliminate the problem of convex combinations in protein sequence pattern discovery. \n"
     ]
    }
   ],
   "source": [
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27645f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c2f8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_text_pubmed(cfg, use_text=False, seed=0):\n",
    "    ######## data, data_pubid \n",
    "    # data, data_pubid = get_pubmed_casestudy(SEED=seed)\n",
    "    path = cfg.dataset.pubmed.original\n",
    "\n",
    "    n_nodes = 19717\n",
    "    n_features = 500\n",
    "\n",
    "    data_X = np.zeros((n_nodes, n_features), dtype='float32')\n",
    "    data_Y = [None] * n_nodes\n",
    "    data_pubid = [None] * n_nodes\n",
    "    data_edges = []\n",
    "\n",
    "    paper_to_index = {}\n",
    "    feature_to_index = {}\n",
    "\n",
    "    # parse nodes\n",
    "    with open(path + 'Pubmed-Diabetes.NODE.paper.tab', 'r') as node_file:\n",
    "        # first two lines are headers\n",
    "        node_file.readline()\n",
    "        node_file.readline()\n",
    "\n",
    "        k = 0\n",
    "\n",
    "        for i, line in enumerate(node_file.readlines()):\n",
    "            items = line.strip().split('\\t')\n",
    "\n",
    "            paper_id = items[0]\n",
    "            data_pubid[i] = paper_id\n",
    "            paper_to_index[paper_id] = i\n",
    "\n",
    "            # label=[1,2,3]\n",
    "            label = int(items[1].split('=')[-1]) - \\\n",
    "                1  # subtract 1 to zero-count\n",
    "            data_Y[i] = label\n",
    "\n",
    "            # f1=val1 \\t f2=val2 \\t ... \\t fn=valn summary=...\n",
    "            features = items[2:-1]\n",
    "            for feature in features:\n",
    "                parts = feature.split('=')\n",
    "                fname = parts[0]\n",
    "                fvalue = float(parts[1])\n",
    "\n",
    "                if fname not in feature_to_index:\n",
    "                    feature_to_index[fname] = k\n",
    "                    k += 1\n",
    "\n",
    "                data_X[i, feature_to_index[fname]] = fvalue\n",
    "\n",
    "    # parse graph\n",
    "    data_A = np.zeros((n_nodes, n_nodes), dtype='float32')\n",
    "\n",
    "    with open(path + 'Pubmed-Diabetes.DIRECTED.cites.tab', 'r') as edge_file:\n",
    "        # first two lines are headers\n",
    "        edge_file.readline()\n",
    "        edge_file.readline()\n",
    "\n",
    "        for i, line in enumerate(edge_file.readlines()):\n",
    "\n",
    "            # edge_id \\t paper:tail \\t | \\t paper:head\n",
    "            items = line.strip().split('\\t')\n",
    "\n",
    "            edge_id = items[0]\n",
    "\n",
    "            tail = items[1].split(':')[-1]\n",
    "            head = items[3].split(':')[-1]\n",
    "\n",
    "            data_A[paper_to_index[tail], paper_to_index[head]] = 1.0\n",
    "            data_A[paper_to_index[head], paper_to_index[tail]] = 1.0\n",
    "            if head != tail:\n",
    "                data_edges.append(\n",
    "                    (paper_to_index[head], paper_to_index[tail]))\n",
    "                data_edges.append(\n",
    "                    (paper_to_index[tail], paper_to_index[head]))\n",
    "              \n",
    "    data_edges = np.unique(data_edges, axis=0).transpose()\n",
    "\n",
    "    ###########\n",
    "    data_X = normalize(data_X, norm=\"l1\")\n",
    "\n",
    "    # load data\n",
    "    data_name = 'PubMed'\n",
    "    # path = osp.join(osp.dirname(osp.realpath(__file__)), 'dataset')\n",
    "    dataset = Planetoid(cfg.dataset.pubmed.root, data_name) # , transform=T.NormalizeFeatures()\n",
    "    data = dataset[0]\n",
    "\n",
    "    # replace dataset matrices with the PubMed-Diabetes data, for which we have the original pubmed IDs\n",
    "    data.x = torch.tensor(data_X)\n",
    "    data.edge_index = torch.tensor(data_edges)\n",
    "    data.y = torch.tensor(data_Y)\n",
    "\n",
    "    # split data\n",
    "    node_id = np.arange(data.num_nodes)\n",
    "    np.random.shuffle(node_id)\n",
    "\n",
    "    data.train_id = np.sort(node_id[:int(data.num_nodes * 0.6)])\n",
    "    data.val_id = np.sort(\n",
    "        node_id[int(data.num_nodes * 0.6):int(data.num_nodes * 0.8)])\n",
    "    data.test_id = np.sort(node_id[int(data.num_nodes * 0.8):])\n",
    "\n",
    "    data.train_mask = torch.tensor(\n",
    "        [x in data.train_id for x in range(data.num_nodes)])\n",
    "    data.val_mask = torch.tensor(\n",
    "        [x in data.val_id for x in range(data.num_nodes)])\n",
    "    data.test_mask = torch.tensor(\n",
    "        [x in data.test_id for x in range(data.num_nodes)])\n",
    "    \n",
    "    ########\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "    \n",
    "    f = open(cfg.dataset.pubmed.abs_ti)\n",
    "    pubmed = json.load(f)\n",
    "    df_pubmed = pd.DataFrame.from_dict(pubmed)\n",
    "\n",
    "    AB = df_pubmed['AB'].fillna(\"\")\n",
    "    TI = df_pubmed['TI'].fillna(\"\")\n",
    "    text = []\n",
    "    for ti, ab in zip(TI, AB):\n",
    "        t = 'Title: ' + ti + '\\n'+'Abstract: ' + ab\n",
    "        text.append(t)        \n",
    "    return data, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01c6ebbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717], train_id=[11830], val_id=[3943], test_id=[3944])\n",
      "Title: Retinal metabolic abnormalities in diabetic mouse: comparison with diabetic rat.\n",
      "Abstract: PURPOSE: Dogs and rats are commonly used to examine the pathogenesis of diabetic retinopathy, but mouse is sparingly studied as an animal model of diabetic retinopathy. In this study metabolic abnormalities, postulated to contribute to the development of retinopathy in diabetes, are investigated in the retina of mice diabetic or galactose-fed for 2 months, and are compared to those obtained from hyperglycemic rats. METHODS: Diabetes was induced in mice (C57BL/6) and rats (Sprague Dawley) by alloxan injection, and experimental galactosemia by feeding normal animals diets supplemented with 30% galactose. After 2 months of hyperglycemia, levels of lipid peroxides, glutathione, nitric oxides and sorbitol, and activities of protein kinase C and (Na-K)-ATPase were measured in the retina. RESULTS: Two months of diabetes or experimental galactosemia in mice increased retinal oxidative stress, PKC activity and nitric oxides by 40-50% and sorbitol levels by 3 folds, and these abnormalities were similar to those observed in the retina of rats hyperglycemic for 2 months. CONCLUSIONS: Metabolic abnormalities, which are postulated to play important role in the development of diabetic retinopathy in other animal models, are present in the retina of diabetic mice, and the level of metabolic abnormalities is very comparable between mice and rats. Thus, mouse seems to be a promising animal model to study the pathogenesis of diabetic retinopathy.\n"
     ]
    }
   ],
   "source": [
    "data, text = get_raw_text_pubmed(cfg, use_text=True, seed=0)\n",
    "print(data)\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52c0818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19717\n"
     ]
    }
   ],
   "source": [
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea4660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_text_arxiv(cfg, use_text=False, seed=0):\n",
    "    # dataset = PygNodePropPredDataset(\n",
    "    #     name='ogbn-arxiv', transform=T.ToSparseTensor())\n",
    "    dataset = PygNodePropPredDataset(\n",
    "        name='ogbn-arxiv')\n",
    "    data = dataset[0]\n",
    "\n",
    "    idx_splits = dataset.get_idx_split()\n",
    "    train_mask = torch.zeros(data.num_nodes).bool()\n",
    "    val_mask = torch.zeros(data.num_nodes).bool()\n",
    "    test_mask = torch.zeros(data.num_nodes).bool()\n",
    "    train_mask[idx_splits['train']] = True\n",
    "    val_mask[idx_splits['valid']] = True\n",
    "    test_mask[idx_splits['test']] = True\n",
    "    data.train_mask = train_mask\n",
    "    data.val_mask = val_mask\n",
    "    data.test_mask = test_mask\n",
    "\n",
    "    # data.edge_index = data.adj_t.to_symmetric()\n",
    "    if not use_text:\n",
    "        return data, None\n",
    "\n",
    "    nodeidx2paperid = pd.read_csv(\n",
    "        'dataset/ogbn_arxiv/mapping/nodeidx2paperid.csv.gz', compression='gzip')\n",
    "\n",
    "    raw_text = pd.read_csv(cfg.dataset.arxiv.abs_ti,\n",
    "                           sep='\\t', header=None, names=['paper id', 'title', 'abs'])\n",
    "\n",
    "    # remove string paper id\n",
    "    nodeidx2paperid['paper id'] = nodeidx2paperid['paper id'].astype(int)\n",
    "    raw_text = raw_text.dropna()\n",
    "    raw_text.loc[1:, 'paper id'] = raw_text[1:]['paper id'].astype(int)\n",
    "    df = pd.merge(nodeidx2paperid, raw_text[1:], on='paper id')\n",
    "    text = []\n",
    "    for ti, ab in zip(df['title'], df['abs']):\n",
    "        t = 'Title: ' + ti + '\\n' + 'Abstract: ' + ab\n",
    "        text.append(t)\n",
    "    return data, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30c1bdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:12<00:00,  6.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/arxiv.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading necessary files...\n",
      "This might take a while.\n",
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10058.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1], train_mask=[169343], val_mask=[169343], test_mask=[169343])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, text = get_raw_text_arxiv(cfg, use_text=True)\n",
    "print(data)\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "874154a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
